<div align="center">

<h1>
SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE
</h1>

<p align="center">
<a href="https://cyw-3d.github.io/projects/SAR3D"><img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome" height=25></a>
<a href="https://arxiv.org/abs/2411.16856"><img src="https://img.shields.io/badge/arXiv-2411.16856-b31b1b?style=for-the-badge&logo=arxiv" height=25></a>
</p>

**[Yongwei ChenÂ¹](https://cyw-3d.github.io) &nbsp;â€¢&nbsp; [Yushi LanÂ¹](https://nirvanalan.github.io) &nbsp;â€¢&nbsp; [Shangchen ZhouÂ¹](https://shangchenzhou.com) &nbsp;â€¢&nbsp; [Tengfei WangÂ²](https://tengfei-wang.github.io) &nbsp;â€¢&nbsp; [Xingang PanÂ¹](https://xingangpan.github.io)**

Â¹S-lab, Nanyang Technological University  
Â²Shanghai Artificial Intelligence Laboratory

**CVPR 2025**

https://github.com/user-attachments/assets/badac244-f8ee-41c2-8129-b09cf6404b91

</div>

## ğŸŒŸ Features
- ğŸ”„ **Autoregressive Modeling**
- âš¡ï¸ **Ultra-fast 3D Generation** (<1s)
- ğŸ” **Detailed Understanding**


## ğŸ› ï¸ Installation & Usage

### Prerequisites

We've tested SAR3D on the following environment:

<details open>
<summary><b>Ubuntu 20.04</b></summary>

- Python 3.9.16
- PyTorch 2.0.0
- CUDA 11.7  
- NVIDIA A6000
</details>

### Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/cyw-3d/SAR3D.git
cd SAR3D
```

2. **Set up environment**
å…ˆé…ç½®ç¯å¢ƒ
```bash
conda env create -f environment.yml
```

å•ç‹¬è£…apex
```
git clone https://github.com/ptrblck/apex.git
cd apex
git checkout apex_no_distributed
pip install -v --no-cache-dir ./
```

å•ç‹¬è£…flash_attn
```
pip install packaging
pip install ninja
pip install flash-attn==2.6.3
```

æˆ–è€…æ‰‹åŠ¨å®‰è£…flash_attn-2.6.3+cuxxtorchx.xcxx11abiFALSE-cp3x-cp3x-linux_x86_64ï¼Œåœ¨https://github.com/Dao-AILab/flash-attention/releases
(å¯¹åº”ç‰ˆæœ¬pytorch_2.0.0-py_3.9-cuda_11.7)
```
wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu118torch2.0cxx11abiFALSE-cp39-cp39-linux_x86_64.whl
pip install flash_attn-2.6.3+cu118torch2.0cxx11abiFALSE-cp39-cp39-linux_x86_64.whl
```

å®‰è£…nvdiffrast
```
git clone https://github.com/NVlabs/nvdiffrast.git
cd nvdiffrast
git checkout v0.3.1
pip install .
```

å®‰è£…libopenblas-dev
```
sudo apt-get update
sudo apt-get install libopenblas-dev
```

å†è£…å…¶ä»–ä¾èµ–
```
pip install -r requirements.txt
```

3. **Download pretrained models** ğŸ“¥

The pretrained models will be automatically downloaded to the `checkpoints` folder during first run.

You can also manually download them from our [model zoo](https://huggingface.co/cyw-3d/sar3d):

| Model | Description | Link |
|-------|-------------|------|
| VQVAE | Base VQVAE model | [vqvae-ckpt.pt](https://huggingface.co/cyw-3d/sar3d/resolve/main/vqvae-ckpt.pt) |
| VQVAE | Flexicubes VQVAE model | [vqvae-flexicubes-ckpt.pt](https://huggingface.co/cyw-3d/sar3d/resolve/main/vqvae-flexicubes-ckpt.pt) |
| Generation | Image-conditioned model | [image-condition-ckpt.pth](https://huggingface.co/cyw-3d/sar3d/resolve/main/image-condition-ckpt.pth) |
| Generation | Text-conditioned model | [text-condition-ckpt.pth](https://huggingface.co/cyw-3d/sar3d/resolve/main/text-condition-ckpt.pth) |

```
mkdir checkpoints
cd checkpoints
wget https://hf-mirror.com/cyw-3d/sar3d/resolve/main/vqvae-flexicubes-ckpt.pt
wget https://hf-mirror.com/cyw-3d/sar3d/resolve/main/vqvae-ckpt.pt
wget https://hf-mirror.com/cyw-3d/sar3d/resolve/main/text-condition-ckpt.pth
wget https://hf-mirror.com/cyw-3d/sar3d/resolve/main/image-condition-ckpt.pth
```


4. **Run inference** ğŸš€
å…ˆå†™jsonä»¥åŠä¸‹è½½CLIPæ¨¡å‹åˆ°æœ¬åœ°ï¼š

åˆ›å»ºjsonæ–‡æ¡£ï¼Œæ”¾ä»¥ä¸‹å†…å®¹ï¼Œå‘½åä¸º`test_text.json`
```
{
    "test_promts": [
      "A small, cute, and round yellow Pikachu stuffed toy with a distinctive yellow color, pointy ears, and large black eyes, resembling the iconic PokÃ©mon character"
    ]
  }

```

ä¸‹è½½CLIPæ¨¡å‹åˆ°æœ¬åœ°ï¼š
```
mkdir pretrained_models
cd pretrained_models
mkdir clip-vit-large-patch14
cd clip-vit-large-patch14
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/config.json
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/vocab.json
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/tokenizer_config.json
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/tokenizer.json
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/tf_model.h5
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/special_tokens_map.json
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/preprocessor_config.json
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/model.safetensors
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/merges.txt
wget https://hf-mirror.com/openai/clip-vit-large-patch14/resolve/main/flax_model.msgpack
```

ä¸‹è½½DINOv2æ¨¡å‹åˆ°æœ¬åœ°ï¼š
```
cd pretrained_models
mkdir dinov2-large
cd dinov2-large
wget https://hf-mirror.com/facebook/dinov2-large/resolve/main/config.json
wget https://hf-mirror.com/facebook/dinov2-large/resolve/main/model.safetensors
wget https://hf-mirror.com/facebook/dinov2-large/resolve/main/preprocessor_config.json
wget https://hf-mirror.com/facebook/dinov2-large/resolve/main/pytorch_model.bin
```

To test the model on your own images:

1. Place your test images in the `test_files/test_images` folder
2. Run the inference script:
```bash
bash test_image.sh
```

To test the model on your own text prompts:

1. Place your test prompts in the `test_files/test_text.json` file
2. Run the inference script:
```bash
bash test_text.sh
```

è¿è¡Œåæ‰“åŒ…
```
chmod +x pack.sh
bash pack.sh
```

## ğŸ“š Training

### Dataset

The dataset is available for download at [Hugging Face](https://huggingface.co/datasets/cyw-3d/sar3d-dataset).

The dataset consists of 8 splits containing preprocessed data based on [G-buffer Objaverse](https://aigc3d.github.io/gobjaverse/), including:
- Rendered images
- Depth maps 
- Camera poses
- Text descriptions
- Normal maps
- Latent embeddings

The dataset covers over 170K unique 3D objects, augmented to more than 630K data pairs. A data.json file is provided that maps object IDs to their corresponding categories.

After downloading and unzipping the dataset, you should have the following structure:
```bash
/dataset-root/
â”œâ”€â”€ 1/
â”œâ”€â”€ 2/
â”œâ”€â”€ ...
â”œâ”€â”€ 8/
â”‚   â””â”€â”€ 0/
â”‚       â”œâ”€â”€ raw_image.png
â”‚       â”œâ”€â”€ depth_alpha.jpg
â”‚       â”œâ”€â”€ c.npy
â”‚       â”œâ”€â”€ caption_3dtopia.txt
â”‚       â”œâ”€â”€ normal.png
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ image_dino_embedding_lrm.npy
â””â”€â”€ dataset.json
```
### Training Commands

The following scripts allow you to train both image-conditioned and text-conditioned models using the dataset stored in the specified `<DATA_DIR>` location.

For image-conditioned model training:
```bash
bash train_image.sh <MODEL_DEPTH> <BATCH_SIZE> <GPU_NUM> <VQVAE_PATH> <OUT_DIR> <DATA_DIR>
```
For text-conditioned model training:
```bash
bash train_text.sh <MODEL_DEPTH> <BATCH_SIZE> <GPU_NUM> <VQVAE_PATH> <OUT_DIR> <DATA_DIR>
```
For VQVAE training
```bash
bash train_VQVAE.sh <DATA_DIR> <GPU_NUM> <BATCH_SIZE> <OUT_DIR>
```

## ğŸ“‹ Roadmap

- [x] Inference and Training Code for Image-conditioned Generation
- [x] Dataset Release
- [x] Inference Code for Text-conditioned Generation
- [x] Training Code for Text-conditioned Generation
- [x] VQVAE training code
- [x] Code for Understanding

## ğŸ“ Citation

If you find this work useful for your research, please cite our paper:

```bibtex
@inproceedings{chen2024sar3d,
    title={SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE},
    author={Chen, Yongwei and Lan, Yushi and Zhou, Shangchen and Wang, Tengfei and Pan, Xingang},
    booktitle={CVPR},
    year={2025}
}
